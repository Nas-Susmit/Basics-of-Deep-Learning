# ğŸ“˜ Understanding Neural Networks With Me

Welcome to Understanding Neural Networks With Me ğŸš€
This repo is a step-by-step implementation of a simple deep learning framework from scratch in Python + NumPy, without using high-level libraries like TensorFlow or PyTorch.

The goal is to learn by building: from data preprocessing, forward & backward propagation, regularization, optimization, all the way to decision boundaries and evaluation metrics.

# âš¡ Features Implemented

âœ”ï¸ Data Loading & Preprocessing

Synthetic datasets (make_moons, make_circles, Gaussian blobs)

Train/dev/test splits

âœ”ï¸ Neural Network Core

He initialization

Forward propagation (ReLU, Sigmoid)

Backward propagation

âœ”ï¸ Gradient Checking

Compare analytical vs numerical gradients

âœ”ï¸ Regularization

L2 regularization

Dropout

âœ”ï¸ Optimization

Mini-batch gradient descent

Momentum

Adam optimizer

Learning rate decay schedules

âœ”ï¸ Visualization

Decision boundaries with matplotlib

Loss curves

âœ”ï¸ Evaluation Metrics

Accuracy, precision, recall, F1-score

Confusion matrix

# ğŸ› ï¸ Installation & Setup

Clone the repo:

git clone https://github.com/YOUR-USERNAME/Basics-of-Deep-Learning.git
cd Basics-of-Deep-Learning


Install dependencies:

pip install -r requirements.txt


# Dependencies:

numpy

matplotlib

scikit-learn

# ğŸ¯ Learning Outcomes

By the end of this project, you will:

Understand how neural networks learn from scratch

See the math behind forward & backward propagation

Implement regularization and optimization strategies

Visualize and evaluate models on toy datasets
